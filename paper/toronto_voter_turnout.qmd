---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - First author
  - Another author
thanks: "Code and data are available at: LINK."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

#install.packages("tinytex")
#install.packages("tidyverse")
#install.packages("knitr")
#install.packages("kableExtra")
#install.packages("here")
#install.packages("ggbeeswarm")

#tinytex::install_tinytex()

#install.packages("rstanarm")
#install.packages("arrow")
#install.packages("tidybayes")
#install.packages("modelsummary")
#install.packages("broom.mixed")
#install.packages("parameters")
#install.packages("sf")
#install.packages("ggrepel")
#install.packages("ggplot2")
#install.packages("dplyr")


library(tidyverse) # a collection of data-related packages
library(knitr) # for kable
library(kableExtra)
library(here)
library(ggbeeswarm)
library(dplyr)
library(ggplot2)
library(sf)
library(ggrepel)
library(rstanarm)
library(arrow)
library(modelsummary)
library(broom.mixed)
library(parameters)


analysis_data <- read_csv(file = here::here("data/analysis_data/analysis_data.csv"))
election_data <- read_csv(file = here::here("data/analysis_data/cleaned_election_data.csv"))
ward_data <- read_csv(file = here::here("data/analysis_data/cleaned_ward_data.csv"))

```

# Introduction

You can and should cross-reference sections and sub-sections. We use @citeR and @rohan.

The remainder of this paper is structured as follows. @sec-data....



# Data {#sec-data}

```{r}
#| echo: false
#| message: false
#| label: tbl-election-data
#| tbl-cap: Sample of Cleaned Elections Data
#| tbl-pos: "h"

#referenced kable stying from https://github.com/christina-wei/INF3104-1-Covid-Clinics/blob/main/outputs/paper/covid_clinics.qmd
head(election_data, 5) |>
  kable(
    col.names = c("Ward ID", "Eligible Voter Turnout (%)"),
    booktabs = TRUE,
    align = "c"
  )

```

```{r}
#| echo: false
#| message: false
#| label: tbl-ward-profile-data
#| tbl-cap: Sample of Cleaned Toronto Ward Profile Data
#| tbl-pos: "h"

head(ward_data, 5) |>
  kable(
    col.names = c("Ward ID", "Uneducated Population (%)", "Unemployment Rate (%)", "Income"),
    booktabs = TRUE,
    align = "c"
  )
```


```{r}
#| echo: false
#| message: false
#| label: tbl-analysis_data
#| tbl-cap: Sample of Combined Ward Election, Income, Employment, and Education Data
#| tbl-pos: "h"

head(analysis_data, 10) |>
  arrange (ward_id) |>
  relocate(c("ward_id", "ward_name", "percent_uneducated", "unemployment_rate", "income", "percent_voted")) |>
  kable(
    booktabs = TRUE,
    col.names = c("Ward ID", "Ward Name", "Uneducated Population (%)", "Unemployment Rate (%)", "Income", "Eligible Voter Turnout (%)"),
    align = c("c", "l", "c", "c", "c", "c")
        ) 
```

```{r}
#| message: false
#| echo: false
#| label: fig-ward-turnout
#| fig-cap: Voter Turnout (%) across Toronto Wards


analysis_data %>%
  ggplot(mapping = aes(x = factor(ward_id), y = percent_voted)) +
  geom_bar(stat = "identity", fill = "gray", color = "black") +
  labs(
    title = "Eligible Voter Turnout (%) across Toronto Wards",
    x = "Ward ID",
    y = "Eligible Voter Turnout (%)"
  ) +
  theme_minimal()
        
```

```{r}
#| tbl-cap: "Summary Statistics"
#| label: tbl-summ-stats
#| echo: false

summary <- analysis_data |>
  summarise(
    Mean_Turnout = mean(percent_voted, na.rm = TRUE),
    Median_Turnout = median(percent_voted, na.rm = TRUE),
    SD_Turnout = sd(percent_voted, na.rm = TRUE),
    Min_Turnout = min(percent_voted, na.rm = TRUE),
    Max_Turnout = max(percent_voted, na.rm = TRUE),
    Mean_Uneducated = mean(percent_uneducated, na.rm = TRUE),
    Median_Uneducated = median(percent_uneducated, na.rm = TRUE),
    SD_Uneducated = sd(percent_uneducated, na.rm = TRUE),
    Min_Uneducated = min(percent_uneducated, na.rm = TRUE),
    Max_Uneducated = max(percent_uneducated, na.rm = TRUE),
    Mean_Unemployment = mean(unemployment_rate, na.rm = TRUE),
    Median_Unemployment = median(unemployment_rate, na.rm = TRUE),
    SD_Unemployment = sd(unemployment_rate, na.rm = TRUE),
    Min_Unemployment = min(unemployment_rate, na.rm = TRUE),
    Max_Unemployment = max(unemployment_rate, na.rm = TRUE),
    Mean_Income = mean(income, na.rm = TRUE),
    Median_Income = median(income, na.rm = TRUE),
    SD_Income = sd(income, na.rm = TRUE),
    Min_Income = min(income, na.rm = TRUE),
    Max_Income = max(income, na.rm = TRUE)
  )
long_summary_stats <-  summary |>
  pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") |>
  separate(Statistic, into = c("Measure", "Variable"), sep = "_") |>
  spread(key = Measure, value = Value) |>
  select(Variable, Mean, Median, SD, Min, Max)

long_summary_stats$Variable <- c("Income", "Voter Turnout (%)", "Uneducated Population (%)", "Unemployment Rate (%)")

kable(long_summary_stats,
      format = "markdown",
      digits = 2,
      col.names = c("Variable", "Mean", "Median", "Standard. Deviation", "Min", "Max"),
      row.names = FALSE,
      align = c('l', 'c', 'c', 'c', 'c', 'c'), 
      )
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-toronto-map
#| fig-cap: "Map of Toronto highlighting the voter turnout across wards"
#| fig.pos: 'H'

# URL to the zip file
url <- "https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/5e7a8234-f805-43ac-820f-03d7c360b588/resource/35f67d86-cfc8-4483-8d77-50d035b010d9/download/25-ward-model-december-2018-wgs84-latitude-longitude.zip"

# Temporary file to store the downloaded zip
temp_zip <- tempfile(fileext = ".zip")

# Download the zip file
download.file(url, temp_zip, mode = "wb")

# Unzip the file to a temporary directory
temp_dir <- tempdir()
unzip(temp_zip, exdir = temp_dir)

# Assuming the shapefile is directly in the unzipped folder and has a standard .shp extension
shapefiles <- list.files(temp_dir, pattern = "\\.shp$", full.names = TRUE)

# Read the shapefile (replace 'shapefiles[1]' with the specific file if there are multiple shapefiles)
toronto_map <- st_read(shapefiles[1], quiet = TRUE)

# Clean up the downloaded zip file
unlink(temp_zip)

# Merge analysis_data with toronto_map by ward_id
toronto_map <- merge(toronto_map, analysis_data, by.x = "AREA_S_CD", by.y = "ward_id")

color_scale <- scale_fill_gradient(low = "lightblue", high = "darkblue", limits = c(22, 38))

# Plot the map
ggplot() +
  geom_sf(data = toronto_map, aes(fill = percent_voted)) +
  color_scale +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



```{r}
#| echo: false
#| label: turnout_vs_income
#| fig-cap: "Correlation between Eligibile Voter Turnout and Ward's Income"
#| warning: false 
  ggplot(data = analysis_data, aes(x = income, y = percent_voted, label = ward_id)) +
  geom_point() +
  geom_text_repel(hjust=-0.5, vjust=0.5) +
  geom_smooth(method = "lm", color = "blue", se = TRUE) +
  theme_minimal() +
  labs(x = "Income", y = "Eligible Voter Turnout (%)") +
  scale_colour_viridis_d()

```

```{r}
#| echo: false
#| label: turnout_vs_unemployment
#| fig-cap: "Correlation between Eligibile Voter Turnout and Ward's Unemployment Rate"
#| warning: false 
  ggplot(data = analysis_data, aes(x = unemployment_rate, y = percent_voted, label = ward_id)) +
  geom_point() +
  geom_text_repel(hjust=-0.5, vjust=0.5) +
  geom_smooth(method = "lm", color = "blue", se = TRUE) +
  theme_minimal() +
  labs(x = "Unemployment Rate (%)", y = "Eligible Voter Turnout (%)") +
  scale_colour_viridis_d()

```

```{r}
#| echo: false
#| label: turnout_vs_education
#| fig-cap: "Correlation between Eligibile Voter Turnout and Ward's Level of Education"
#| warning: false 
  ggplot(data = analysis_data, aes(x = percent_uneducated, y = percent_voted, label = ward_id)) +
  geom_point() +
  geom_text_repel(hjust=-0.5, vjust=0.5) +
  geom_smooth(method = "lm", color = "blue", se = TRUE) +
  theme_minimal() +
  labs(x = "Uneducated Population (%)", y = "Eligible Voter Turnout (%)") +
  scale_colour_viridis_d()

```

# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/poisson_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```




# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]


analysis_data <- read_csv(file = here::here("data/analysis_data/analysis_data.csv"))
analysis_data <- analysis_data %>%
  mutate(income = income / 1000, percent_voted = as.integer(round(percent_voted)))

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```



\newpage


# References


